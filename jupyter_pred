import pandas as pd
import matplotlib.pyplot as plt
import requests
import jpholiday
from datetime import timedelta
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
features = [
    "temperature_2m_max", "temperature_2m_min", "precipitation_sum", "precipitation_hours",
    "windspeed_10m_mean", "windspeed_10m_max", "shortwave_radiation_sum",
    "temp_10d_avg", 
    "is_weekend", "is_holiday_flag", "is_day_before_holiday",
    "relative_humidity_2m_max", "relative_humidity_2m_min", "et0_fao_evapotranspiration",
    "windgusts_10m_max", "weekday", "month","temp2","prc2","temperature_2m_mean","tempprc","shiny_holiday",
    "et02"
] 
# â–  ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆmacOS å‘ã‘ï¼‰
plt.rcParams["font.family"] = "Hiragino Sans"
plt.rcParams["axes.unicode_minus"] = False

# â–  ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv("/Users/nozakizai/Documents/ã‚¯ãƒ©ãƒ•ãƒˆãƒ’ã‚™ãƒ¼ãƒ«å°‚é–€åº—_å£²ä¸Šãƒ†ã‚™ãƒ¼ã‚¿_2024å¹´4æœˆ-2025å¹´4æœˆ (æ›œæ—¥ã‚ã‚Šãƒ†ã‚™ãƒ¼ã‚¿).csv")  # ãƒ‘ã‚¹ã‚’é©å®œå¤‰æ›´
df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")

# â–  å£²ä¸Šåˆ—æ•´å½¢
numeric_cols = [col for col in df.columns if "(å††)" in col or "(æœ¬)" in col]
for col in numeric_cols:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace(",", "", regex=False), errors="coerce").fillna(0)

# â–  æ—¥ä»˜æƒ…å ±
df["weekday"] = df["æ—¥ä»˜"].dt.day_name()
df["month"] = df["æ—¥ä»˜"].dt.month_name()


# -------------------------------
# â–  å¤©æ°—ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆOpen-Meteo APIï¼‰
# -------------------------------
lat, lon = 35.6895, 139.6917
start_date = df["æ—¥ä»˜"].min().strftime("%Y-%m-%d")
end_date = df["æ—¥ä»˜"].max().strftime("%Y-%m-%d")

url = (
    f"https://archive-api.open-meteo.com/v1/archive?"
    f"latitude={lat}&longitude={lon}&start_date={start_date}&end_date={end_date}"
    f"&daily=temperature_2m_max,temperature_2m_min,temperature_2m_mean,"
    f"precipitation_sum,precipitation_hours,relative_humidity_2m_max,relative_humidity_2m_min,"
    f"windspeed_10m_max,windspeed_10m_mean,windgusts_10m_max,"
    f"shortwave_radiation_sum,et0_fao_evapotranspiration,"
    f"sunset,"
    f"uv_index_max&timezone=Asia%2FTokyo"
)
weather_df = pd.DataFrame(requests.get(url).json()["daily"])
weather_df["æ—¥ä»˜"] = pd.to_datetime(weather_df["time"])

# â–  ãƒ“ãƒ¼ãƒ«ã®åˆ—å®šç¾©
beer_columns = ["ãƒšãƒ¼ãƒ«ã‚¨ãƒ¼ãƒ«(æœ¬)", "ãƒ©ã‚¬ãƒ¼(æœ¬)", "IPA(æœ¬)", "ãƒ›ãƒ¯ã‚¤ãƒˆãƒ“ãƒ¼ãƒ«(æœ¬)", "é»’ãƒ“ãƒ¼ãƒ«(æœ¬)", "ãƒ•ãƒ«ãƒ¼ãƒ„ãƒ“ãƒ¼ãƒ«(æœ¬)"]
beer_prices = ["ãƒšãƒ¼ãƒ«ã‚¨ãƒ¼ãƒ«(å††)", "ãƒ©ã‚¬ãƒ¼(å††)", "IPA(å††)", "ãƒ›ãƒ¯ã‚¤ãƒˆãƒ“ãƒ¼ãƒ«(å††)", "é»’ãƒ“ãƒ¼ãƒ«(å††)", "ãƒ•ãƒ«ãƒ¼ãƒ„ãƒ“ãƒ¼ãƒ«(å††)","ç·æ¯æ•°","å£²ä¸Šåˆè¨ˆ(å††)","æ¥å®¢æ•°"]
for col in beer_columns:
    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)

# â–  å£²ä¸Šã¨å¤©æ°—ã‚’ãƒãƒ¼ã‚¸
merged_df = pd.merge(df, weather_df, on="æ—¥ä»˜", how="inner")
merged_df["weekday"] = merged_df["æ—¥ä»˜"].dt.weekday
merged_df["month"] = merged_df["æ—¥ä»˜"].dt.month


merged_df["tempprc"] = merged_df["temperature_2m_mean"]*merged_df["precipitation_sum"]
merged_df["temp2"] = merged_df["temperature_2m_mean"]*merged_df["temperature_2m_mean"]
merged_df["prc2"] = merged_df["precipitation_sum"]*merged_df["precipitation_sum"]
merged_df["et02"] = merged_df["et0_fao_evapotranspiration"]*merged_df["et0_fao_evapotranspiration"]

# â–  å¹³å‡æ°—æ¸©ã¨ã®ä¹–é›¢
for window_size in range(1, 15):  # 1æ—¥ã‹ã‚‰14æ—¥ã¾ã§
    col_avg = f"temp_{window_size}d_avg"
    col_diff = f"temp_diff_{window_size}d"
    merged_df[col_avg] = merged_df["temperature_2m_mean"].rolling(window=window_size, min_periods=1).mean()
    merged_df[col_diff] = merged_df["temperature_2m_mean"] - merged_df[col_avg]

# precipitation_sumã®ç§»å‹•å¹³å‡ã¨å·®åˆ†
for window_size in range(1, 15):
    prec_avg_col = f"precip_{window_size}d_avg"
    prec_diff_col = f"precip_diff_{window_size}d"
    merged_df[prec_avg_col] = merged_df["precipitation_sum"].rolling(window=window_size, min_periods=1).mean()
    merged_df[prec_diff_col] = merged_df["precipitation_sum"] - merged_df[prec_avg_col]
    features.append(prec_diff_col)
    
for window_size in range(1, 15):  # temperatureå·®åˆ†ã®2ä¹—åˆ—
    col_diff = f"temp_diff_{window_size}d"
    col_diff_sq = f"{col_diff}_squared"
    merged_df[col_diff_sq] = merged_df[col_diff] ** 2

for window_size in range(1, 15):  # precipitationå·®åˆ†ã®2ä¹—åˆ—
    prec_diff_col = f"precip_diff_{window_size}d"
    prec_diff_sq_col = f"{prec_diff_col}_squared"
    merged_df[prec_diff_sq_col] = merged_df[prec_diff_col] ** 2
    features.append(prec_diff_sq_col)  # å¿…è¦ãªã‚‰featuresã«ã‚‚è¿½åŠ 




# â–  ãƒ•ãƒ©ã‚°ç³»ï¼ˆä¼‘æ—¥ã€é¢¨å¼·ã€å‰æ—¥ä¼‘æ—¥ï¼‰
merged_df["is_holiday"] = merged_df["æ—¥ä»˜"].apply(jpholiday.is_holiday)
merged_df["is_weekend"] = merged_df["æ—¥ä»˜"].dt.weekday >= 5
merged_df["is_newyear"] = merged_df["æ—¥ä»˜"].apply(lambda d: (d.month == 12 and d.day == 31) or (d.month == 1 and d.day in [1,2,3]))
merged_df["is_holiday_flag"] = merged_df[["is_holiday", "is_weekend", "is_newyear"]].any(axis=1).astype(int)

def is_day_before_holiday(date):
    next_day = date + timedelta(days=1)
    return jpholiday.is_holiday(next_day) or next_day.weekday() >= 5 or (next_day.month == 12 and next_day.day == 31)

merged_df["is_day_before_holiday"] = merged_df["æ—¥ä»˜"].apply(is_day_before_holiday).astype(int)

# è«–ç†å€¤ã‚’æ•´æ•°ã«å¤‰æ›
bool_cols = merged_df.select_dtypes(include="bool").columns
merged_df[bool_cols] = merged_df[bool_cols].astype(int)
merged_df["shiny_holiday"]=merged_df["is_holiday_flag"]* merged_df["shortwave_radiation_sum"]
# -------------------------------
# â–  ç‰¹å¾´é‡ãƒªã‚¹ãƒˆï¼ˆæ˜ç¤ºï¼‰
# -------------------------------

# 1ã€œ14æ—¥ã®å·®åˆ†åˆ—åã‚’ä½œã£ã¦featuresã«è¿½åŠ 
for window_size in range(1, 15):
    col_diff = f"temp_diff_{window_size}d"
    features.append(col_diff)

# -------------------------------
# âœ… å‡ºåŠ›ç¢ºèª
# -------------------------------
print(merged_df[["æ—¥ä»˜"] + features].head())


# å…¨åˆ—åã‚’è¡¨ç¤º
for col in merged_df.columns:
    print(col)

print(merged_df["windspeed_10m_mean"])


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from prophet import Prophet
from sklearn.impute import SimpleImputer
import joblib
import json
import pickle
import os
import optuna
from prophet.serialize import model_to_json

with open('serialized_model.json', 'w') as fout:
    json.dump(model_to_json(model_prophet), fout)


# ä½¿ç”¨ã™ã‚‹ç›®çš„å¤‰æ•°ï¼ˆæ—¥æœ¬èªï¼‰
target_cols = ["ãƒšãƒ¼ãƒ«ã‚¨ãƒ¼ãƒ«(æœ¬)", "ãƒ©ã‚¬ãƒ¼(æœ¬)", "IPA(æœ¬)", "ãƒ›ãƒ¯ã‚¤ãƒˆãƒ“ãƒ¼ãƒ«(æœ¬)", "é»’ãƒ“ãƒ¼ãƒ«(æœ¬)", "ãƒ•ãƒ«ãƒ¼ãƒ„ãƒ“ãƒ¼ãƒ«(æœ¬)"]

# æ—¥æœ¬èªâ†’è‹±èªã®å¤‰æ›è¾æ›¸ï¼ˆä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€åã«ä½¿ç”¨ï¼‰
beer_name_map = {
    "ãƒšãƒ¼ãƒ«ã‚¨ãƒ¼ãƒ«(æœ¬)": "pale_ale",
    "ãƒ©ã‚¬ãƒ¼(æœ¬)": "lager",
    "IPA(æœ¬)": "ipa",
    "ãƒ›ãƒ¯ã‚¤ãƒˆãƒ“ãƒ¼ãƒ«(æœ¬)": "whitebeer",
    "é»’ãƒ“ãƒ¼ãƒ«(æœ¬)": "dark",
    "ãƒ•ãƒ«ãƒ¼ãƒ„ãƒ“ãƒ¼ãƒ«(æœ¬)": "fruit"
}

# ä½¿ç”¨ã™ã‚‹åˆ—ï¼ˆfeaturesã¯å®šç¾©æ¸ˆã¿ã®å‰æï¼‰
used_columns = features + target_cols + ["æ—¥ä»˜"]
merged_df = merged_df[used_columns]

# Train/test åˆ†å‰²
train_end = int(len(merged_df) * 0.85)

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ã®ã‚·ãƒ¼ãƒ‰
num_seeds = 10
seeds = [41, 42, 43, 44, 45, 46, 47, 48, 49, 50]

# ç›®çš„å¤‰æ•°ãƒ«ãƒ¼ãƒ—
for target_col in target_cols:
    print("=" * 70)
    print(f"ğŸ¯ ç›®çš„å¤‰æ•°: {target_col}")
    
    # ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    beer_name_en = beer_name_map[target_col]
    model_dir = os.path.join("ultra_models", beer_name_en)
    os.makedirs(model_dir, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    y = merged_df[target_col]
    all_feature_cols = features
    y_log = np.log1p(y)

    X_train = merged_df.iloc[:train_end][all_feature_cols]
    y_train_log = y_log.iloc[:train_end]

    X_test = merged_df.iloc[train_end:][all_feature_cols]
    y_test_log = y_log.iloc[train_end:]
    y_test_original = y.iloc[train_end:]
    date_test = merged_df.iloc[train_end:]["æ—¥ä»˜"]

    # æ¬ æè£œå®Œ
    imputer = SimpleImputer(strategy="median")
    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
    X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

    # Optuna ã§ LightGBM ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
    def objective(trial):
        params = {
            "objective": "regression",
            "metric": "rmse",
            "boosting_type": "gbdt",
            "learning_rate": trial.suggest_loguniform("learning_rate", 0.005, 0.1),
            "num_leaves": trial.suggest_int("num_leaves", 20, 100),
            "max_depth": trial.suggest_int("max_depth", 3, 15),
            "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 100),
            "feature_fraction": trial.suggest_uniform("feature_fraction", 0.6, 1.0),
            "bagging_fraction": trial.suggest_uniform("bagging_fraction", 0.6, 1.0),
            "bagging_freq": trial.suggest_int("bagging_freq", 1, 10),
            "lambda_l1": trial.suggest_loguniform("lambda_l1", 1e-8, 10.0),
            "lambda_l2": trial.suggest_loguniform("lambda_l2", 1e-8, 10.0),
            "verbosity": -1,
            "seed": 42,
        }
        lgb_train = lgb.Dataset(X_train_imputed, label=y_train_log)
        gbm = lgb.train(params, lgb_train, num_boost_round=1000, valid_sets=[lgb_train])
        y_pred = gbm.predict(X_test_imputed)
        rmse = np.sqrt(mean_squared_error(y_test_log, y_pred))
        return rmse

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=100, show_progress_bar=True)

    best_params = study.best_params
    print(f"âœ¨ Best params: {best_params}")
    print(f"âœ¨ Best RMSE: {study.best_value:.4f}")

    # LightGBM å­¦ç¿’ãƒ»ä¿å­˜ï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒ‰ï¼‰
    lgb_preds = []
    for seed in seeds:
        best_params["seed"] = seed
        lgb_train = lgb.Dataset(X_train_imputed, label=y_train_log)
        model_lgb = lgb.train(best_params, lgb_train, num_boost_round=1000, valid_sets=[lgb_train])
        y_pred_lgb_log = model_lgb.predict(X_test_imputed)
        y_pred_lgb = np.expm1(y_pred_lgb_log)
        lgb_preds.append(y_pred_lgb)
        joblib.dump(model_lgb, os.path.join(model_dir, f"lgb_seed_{seed}.pkl"))

    y_pred_lgb_ensemble = np.mean(lgb_preds, axis=0)

    # ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«
    model_lr = LinearRegression()
    model_lr.fit(X_train_imputed, y_train_log)
    y_pred_lr_log = model_lr.predict(X_test_imputed)
    y_pred_lr = np.expm1(y_pred_lr_log)
    joblib.dump(model_lr, os.path.join(model_dir, "linear_model.pkl"))

    # Prophet ãƒ¢ãƒ‡ãƒ«
# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    prophet_df = pd.DataFrame({"ds": merged_df["æ—¥ä»˜"][:train_end], "y": y_train_log})

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆã¨å­¦ç¿’
    model_prophet = Prophet()
    model_prophet.fit(prophet_df)
    # äºˆæ¸¬å¯¾è±¡æ—¥ä»˜ã®DataFrameä½œæˆ
    future_df = pd.DataFrame({"ds": date_test})
    prophet_forecast = model_prophet.predict(future_df)

    # äºˆæ¸¬çµæœã‚’å¤‰æ›
    y_pred_prophet_log = prophet_forecast["yhat"].values
    y_pred_prophet = np.expm1(y_pred_prophet_log)

    import numpy as np


    def convert_np_types(obj):
        if isinstance(obj, dict):
            return {k: convert_np_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_np_types(i) for i in obj]
        elif isinstance(obj, (np.integer,)):
            return int(obj)
        elif isinstance(obj, (np.floating,)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆNumPyå‹ã‚’å›é¿ã—ã¦ä¿å­˜ï¼‰
    with open(os.path.join(model_dir, "prophet_model.json"), "w") as f:
        json.dump(convert_np_types(model_to_json(model_prophet)), f)


    # å˜ç´”å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    y_pred_ensemble = (y_pred_lgb_ensemble + y_pred_lr + y_pred_prophet) / 3

    # ã‚¹ã‚³ã‚¢
    rmse_test = np.sqrt(mean_squared_error(y_test_original, y_pred_ensemble))
    r2_test = r2_score(y_test_original, y_pred_ensemble)
    mae_test = mean_absolute_error(y_test_original, y_pred_ensemble)
    print(f"âœ… [TEST-Ensemble] RMSE: {rmse_test:.2f} / RÂ²: {r2_test:.3f} / MAE: {mae_test:.2f}")

    # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆæœ€å¾Œã®LightGBMï¼‰
    importance = model_lgb.feature_importance(importance_type="gain")
    feature_names = model_lgb.feature_name()
    importance_df = pd.DataFrame({"feature": feature_names, "importance": importance}).sort_values(by="importance", ascending=False)
    print("â–¶ï¸ ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆGainé †ï¼‰")
    print(importance_df)

    # å¯è¦–åŒ–ï¼ˆãƒˆãƒƒãƒ—20ç‰¹å¾´é‡ï¼‰
    top_n = 20
    plt.figure(figsize=(8, 6))
    plt.barh(importance_df["feature"].head(top_n)[::-1], importance_df["importance"].head(top_n)[::-1], color="skyblue")
    plt.title(f"Feature Importance for {target_col}")
    plt.xlabel("Gain Importance")
    plt.tight_layout()
    plt.show()

    # é€±å˜ä½è©•ä¾¡
    date_test = pd.to_datetime(date_test)
    weekly_df = pd.DataFrame({"date": date_test, "actual": y_test_original.values, "predicted": y_pred_ensemble})
    weekly_df["week_start"] = weekly_df["date"] - pd.to_timedelta(weekly_df["date"].dt.weekday, unit='d')
    weekly_summary = weekly_df.groupby("week_start")[["actual", "predicted"]].sum().reset_index()
    weekly_rmse = np.sqrt(mean_squared_error(weekly_summary["actual"], weekly_summary["predicted"]))
    weekly_r2 = r2_score(weekly_summary["actual"], weekly_summary["predicted"])
    weekly_mae = mean_absolute_error(weekly_summary["actual"], weekly_summary["predicted"])
    print(f"ğŸ“… [TEST-Ensemble / Weekly Sum] RMSE: {weekly_rmse:.2f} / RÂ²: {weekly_r2:.3f} / MAE: {weekly_mae:.2f}")



